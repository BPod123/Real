{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "train_percent = 0.8\n",
    "validation_percent = 0.1\n",
    "test_percent = 0.1\n",
    "vocab_cutoff = 2\n",
    "\n",
    "random_seed = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from HeadlineDataset import HeadlineDataset\n",
    "from torch.utils.data import RandomSampler, DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = (\n",
    "        text.replace(\"#39;\", \"'\")\n",
    "        .replace(\"amp;\", \"&\")\n",
    "        .replace(\"#146;\", \"'\")\n",
    "        .replace(\"nbsp;\", \" \")\n",
    "        .replace(\"#36;\", \"$\")\n",
    "        .replace(\"\\\\n\", \"\\n\")\n",
    "        .replace(\"quot;\", \"'\")\n",
    "        .replace(\"<br />\", \"\\n\")\n",
    "        .replace('\\\\\"', '\"')\n",
    "        .replace(\" @.@ \", \".\")\n",
    "        .replace(\" @-@ \", \"-\")\n",
    "        .replace(\" @,@ \", \",\")\n",
    "        .replace(\"\\\\\", \" \\\\ \")\n",
    "    )\n",
    "    text = re.compile(r\"  +\").sub(\" \", html.unescape(text))\n",
    "    text = re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", text)\n",
    "    text =  re.sub(r\"([/#\\n])\", r\" \\1 \", text)\n",
    "    text = re.sub(\" {2,}\", \" \", text).strip()\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"../Data/Headlines.db\")\n",
    "\n",
    "fake_headlines = pd.read_sql(\"SELECT * FROM fake_headlines\", con)\n",
    "\n",
    "real_headlines = pd.read_sql(\"\"\"\n",
    "select headline.title as text\n",
    "from headline join feed f on headline.url = f.url\n",
    "WHERE f.name not in (\"The Onion (Fake News)\", \"Babylon Bee (Fake News)\")\n",
    "\"\"\", con)\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "del seed\n",
    "fake_headlines[\"tokenized\"] = fake_headlines[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower()))).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "real_headlines[\"tokenized\"] = real_headlines[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower()))).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "fake_headlines['label'] = 1\n",
    "real_headlines['label'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Up Data\n",
    "80% for training, 10% for validation, 10% for testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0% for training, 1.0% for validation, and 1.0% for testing\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train_percent * 10}% for training, {validation_percent * 10}% for validation, and {test_percent * 10}% for testing\")\n",
    "def split_train_val_test(df, props):\n",
    "    assert round(sum(props), 2) == 1\n",
    "    results = []\n",
    "    start = 0\n",
    "    for percent in props:\n",
    "        length = int(len(df) * percent)\n",
    "        end = start + length + 1\n",
    "        results.append(df.iloc[start:end])\n",
    "        start = end\n",
    "    return results\n",
    "props = [train_percent, validation_percent, test_percent]\n",
    "fake_sets = split_train_val_test(fake_headlines, props)\n",
    "for i in range(len(fake_sets)):\n",
    "    for j in range(4):\n",
    "        fake_sets[i] = pd.concat([fake_sets[i], fake_sets[i]])\n",
    "real_sets = split_train_val_test(real_headlines, props)\n",
    "\n",
    "train_set, validation_set, test_set = [pd.concat([x, y]).sample(frac=1).reset_index(drop=True) for x, y in zip(fake_sets, real_sets)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training headlines: 429089\n",
      "validation headlines: 53643\n",
      "test headlines: 53624\n"
     ]
    }
   ],
   "source": [
    "print(f\"training headlines: {train_set.shape[0]}\")\n",
    "print(f\"validation headlines: {validation_set.shape[0]}\")\n",
    "print(f\"test headlines: {test_set.shape[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length: 40023\n"
     ]
    }
   ],
   "source": [
    "PAD, UNK = 0, 1\n",
    "def count_tokens(df):\n",
    "    print(\"starting\")\n",
    "    count = Counter()\n",
    "    for tokens in df[\"tokenized\"]:\n",
    "        count += Counter(tokens)\n",
    "    return count\n",
    "def generate_vocab_map(df, cutoff):\n",
    "    vocab          = {\"\": PAD, \"UNK\": UNK}\n",
    "    reversed_vocab = {PAD: \"\", UNK: \"UNK\"}\n",
    "    count = Counter(np.concatenate(df['tokenized'].to_numpy()))\n",
    "    keep_keys = [key for key in count if count[key] > cutoff]\n",
    "    for i in range(len(keep_keys)):\n",
    "        key = keep_keys[i]\n",
    "        vocab[key] = i + 2\n",
    "        reversed_vocab[i + 2] = key\n",
    "\n",
    "    return vocab, reversed_vocab\n",
    "vocab, reversed_vocab = generate_vocab_map(train_set, cutoff=2)\n",
    "print(f\"Vocab Length: {len(vocab)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Store the vocab and reversed_vocab for later use after training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open('../Data/vocab.json', 'w') as vocab_file:\n",
    "    vocab_file.write(json.dumps(vocab))\n",
    "with open('../Data/reversed_vocab.json', 'w') as reversed_vocab_file:\n",
    "    reversed_vocab_file.write(json.dumps(reversed_vocab))\n",
    "del vocab_file, reversed_vocab_file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_data = HeadlineDataset(vocab, train_set)\n",
    "validation_data = HeadlineDataset(vocab, validation_set)\n",
    "test_data = HeadlineDataset(vocab, validation_set)\n",
    "\n",
    "\n",
    "\n",
    "collate_func = lambda batch: (pad_sequence([x[0] for x in batch], padding_value=PAD, batch_first=True).to(device), torch.FloatTensor([x[1] for x in batch]).to(device))\n",
    "\n",
    "\n",
    "def create_data_iterators(batch_size):\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    validation_sampler = RandomSampler(validation_data)\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    train_iterator = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=collate_func)\n",
    "    validation_iterator = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size, collate_fn=collate_func)\n",
    "    test_iterator = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size, collate_fn=collate_func)\n",
    "    return train_iterator, validation_iterator, test_iterator\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and Validation Loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def training_loop(model, loss_func, optimizer, iterator):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data, labels in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        if out.shape != labels.shape:\n",
    "            continue\n",
    "        loss = loss_func(out, labels)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss\n",
    "\n",
    "def validation_loop(model, iterator):\n",
    "    predictions, labels = [], []\n",
    "    validator = ((data, new_labels) for data, new_labels in iterator if predictions.append(model(data).round()) is None and (labels.append(new_labels)) is None and False)\n",
    "    list(validator)\n",
    "    predictions, labels = torch.round(torch.cat(predictions).squeeze()).to(device), torch.round(torch.cat(labels).squeeze()).to(device)\n",
    "    return labels, predictions\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def accuracy(true, pred):\n",
    "    tp = ((pred == True) & (true == True)).sum()\n",
    "    tn = ((pred == False) & (true == False)).sum()\n",
    "    return (tp + tn).sum() / len(true)\n",
    "def binary_f1(true, pred, selected_class=True):\n",
    "    tp = ((pred == selected_class) & (true == selected_class)).sum()\n",
    "    fp = ((pred == selected_class) & (true != selected_class)).sum()\n",
    "    fn = ((pred != selected_class) & (true == selected_class)).sum()\n",
    "    if tp + fn == 0 or tp + fp == 0:\n",
    "        return 0\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def binary_macro_f1(true, pred):\n",
    "    return (binary_f1(true, pred, True) + binary_f1(true, pred, False)) / 2\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "        labels, predictions  = validation_loop(model, iterator)\n",
    "        f1 = binary_macro_f1(labels, predictions)\n",
    "        acc = accuracy(labels, predictions)\n",
    "        return f1, acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_model(model, loss_func, optimizer, epochs, train_iterator, validation_iterator):\n",
    "    validation_labels, validation_predictions = validation_loop(model, validation_iterator)\n",
    "    validation_f1_pre_train = binary_macro_f1(validation_labels, validation_predictions)\n",
    "    validation_accuracy_pre_train = accuracy(validation_predictions, validation_labels)\n",
    "    print(f\"\"\"\n",
    "Before Training\n",
    "F1: {validation_f1_pre_train}\n",
    "Accuracy: {validation_accuracy_pre_train}\n",
    "\"\"\")\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = training_loop(model, loss_func, optimizer, train_iterator)\n",
    "\n",
    "        print(f\"EPOCH: {epoch}\")\n",
    "        print(f\"TRAIN LOSS: {train_loss}\")\n",
    "        f1, acc = evaluate(model, validation_iterator)\n",
    "        print(f\"VAL F-1: {f1}\")\n",
    "        print(f\"VAL ACC: {acc}\")\n",
    "# def train_epoch(model, loss_func, )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1\n",
    "## Neural Bag Of Words ([NBOW](https://www.aclweb.org/anthology/P15-1162.pdf))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def train_method_1(model, loss_func, optimizer, scheduler, epochs, train_iterator, validation_iterator):\n",
    "    validation_labels, validation_predictions = validation_loop(model, validation_iterator)\n",
    "    validation_f1_pre_train = binary_macro_f1(validation_labels, validation_predictions)\n",
    "    validation_accuracy_pre_train = accuracy(validation_predictions, validation_labels)\n",
    "\n",
    "    print(f\"\"\"\n",
    "Before Training\n",
    "F1: {validation_f1_pre_train}\n",
    "Accuracy: {validation_accuracy_pre_train}\n",
    "\"\"\")\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = training_loop(model, loss_func, optimizer, train_iterator)\n",
    "        val_labels, val_predictions  = validation_loop(model, validation_iterator)\n",
    "        print(f\"EPOCH: {epoch + 1}\")\n",
    "        print(f\"TRAIN LOSS: {train_loss}\")\n",
    "        f1 = binary_macro_f1(val_labels, val_predictions)\n",
    "        acc = accuracy(val_labels, val_predictions)\n",
    "        print(f\"VAL F-1: {f1}\")\n",
    "        print(f\"VAL ACC: {acc}\")\n",
    "        scheduler.step()\n",
    "        # scheduler.step(loss_func(val_predictions, val_labels))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before Training\n",
      "F1: 0.2653433084487915\n",
      "Accuracy: 0.3610536456108093\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:37<00:00, 275.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "TRAIN LOSS: 8170.75439453125\n",
      "VAL F-1: 0.8089728355407715\n",
      "VAL ACC: 0.8366050124168396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 286.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2\n",
      "TRAIN LOSS: 2738.53662109375\n",
      "VAL F-1: 0.8093274831771851\n",
      "VAL ACC: 0.8400909900665283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 285.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3\n",
      "TRAIN LOSS: 2174.182373046875\n",
      "VAL F-1: 0.8140965700149536\n",
      "VAL ACC: 0.8432227969169617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 287.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4\n",
      "TRAIN LOSS: 2114.98681640625\n",
      "VAL F-1: 0.8151694536209106\n",
      "VAL ACC: 0.8441548943519592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:32<00:00, 288.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5\n",
      "TRAIN LOSS: 2116.216796875\n",
      "VAL F-1: 0.8161953091621399\n",
      "VAL ACC: 0.8449005484580994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 286.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6\n",
      "TRAIN LOSS: 2102.933837890625\n",
      "VAL F-1: 0.8153846263885498\n",
      "VAL ACC: 0.8440616726875305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 287.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7\n",
      "TRAIN LOSS: 2103.52197265625\n",
      "VAL F-1: 0.816043496131897\n",
      "VAL ACC: 0.8447887301445007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:34<00:00, 284.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8\n",
      "TRAIN LOSS: 2108.86279296875\n",
      "VAL F-1: 0.8170247673988342\n",
      "VAL ACC: 0.8453852534294128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:32<00:00, 288.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9\n",
      "TRAIN LOSS: 2105.103271484375\n",
      "VAL F-1: 0.8177109956741333\n",
      "VAL ACC: 0.8460936546325684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 286.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10\n",
      "TRAIN LOSS: 2108.978271484375\n",
      "VAL F-1: 0.8170309662818909\n",
      "VAL ACC: 0.8454039096832275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 287.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11\n",
      "TRAIN LOSS: 2101.121826171875\n",
      "VAL F-1: 0.8171756863594055\n",
      "VAL ACC: 0.8455530405044556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 286.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12\n",
      "TRAIN LOSS: 2109.872314453125\n",
      "VAL F-1: 0.8149775266647339\n",
      "VAL ACC: 0.8438566327095032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:32<00:00, 290.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13\n",
      "TRAIN LOSS: 2109.208740234375\n",
      "VAL F-1: 0.8152182698249817\n",
      "VAL ACC: 0.8440244197845459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:32<00:00, 288.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14\n",
      "TRAIN LOSS: 2105.4072265625\n",
      "VAL F-1: 0.8161212205886841\n",
      "VAL ACC: 0.8447514176368713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 286.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15\n",
      "TRAIN LOSS: 2104.37548828125\n",
      "VAL F-1: 0.8160717487335205\n",
      "VAL ACC: 0.8446582555770874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:32<00:00, 289.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16\n",
      "TRAIN LOSS: 2109.271240234375\n",
      "VAL F-1: 0.8163934946060181\n",
      "VAL ACC: 0.8449192047119141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:32<00:00, 291.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17\n",
      "TRAIN LOSS: 2112.466064453125\n",
      "VAL F-1: 0.8172512650489807\n",
      "VAL ACC: 0.8456276059150696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:34<00:00, 285.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18\n",
      "TRAIN LOSS: 2110.344482421875\n",
      "VAL F-1: 0.8162152767181396\n",
      "VAL ACC: 0.8448632955551147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 287.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19\n",
      "TRAIN LOSS: 2105.7822265625\n",
      "VAL F-1: 0.8156701326370239\n",
      "VAL ACC: 0.8445091247558594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26819/26819 [01:33<00:00, 286.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20\n",
      "TRAIN LOSS: 2103.451416015625\n",
      "VAL F-1: 0.8155826330184937\n",
      "VAL ACC: 0.844397246837616\n"
     ]
    }
   ],
   "source": [
    "from NBOW import NBOW\n",
    "model_1 = NBOW(len(vocab.keys()), 300).to(device)\n",
    "model_1_epochs = 20\n",
    "model_1_batch_size = 16\n",
    "model_1_optimizer = Adam(model_1.parameters(), lr=0.05)\n",
    "# loss_func_1 = nn.BCEWithLogitsLoss()\n",
    "loss_func_1 = nn.BCELoss()\n",
    "# scheduler_1 = ReduceLROnPlateau(model_1_optimizer, mode='min')\n",
    "scheduler_1 = ExponentialLR(model_1_optimizer, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "model_1_train, model_1_validation, model_1_test = create_data_iterators(model_1_batch_size)\n",
    "train_method_1(model_1, loss_func_1, model_1_optimizer, scheduler_1, model_1_epochs, model_1_train, model_1_validation)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test data\n",
      "Test F-1: 0.81651771068573\n",
      "Test ACC: 0.8451429009437561\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluation on test data\")\n",
    "f1, acc = evaluate(model_1, model_1_test)\n",
    "print(f\"Test F-1: {f1}\")\n",
    "print(f\"Test ACC: {acc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), \"NBOW.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}